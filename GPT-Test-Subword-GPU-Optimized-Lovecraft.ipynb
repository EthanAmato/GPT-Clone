{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a17808b-7dcd-41bd-8ae3-43ba3643f4ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5563fc71-0678-47e0-8041-944eb168e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d90f90ac-723c-44fc-9081-86d2cd482b65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch # lets use pytorch\n",
    "import tiktoken \n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b27f00-6021-466a-ae56-f160c9c3139b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4f48a9e-21b0-4925-96ac-7087c9278e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "\n",
    "#384 // 6 = 64 dimensional as standard for each head\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d789a-97e6-479e-af24-52e83d6d4058",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be8dec6c-b6f5-4e22-93ea-95eb5c48dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/little-lovecraft.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "486a35a8-e44c-44f3-a5fd-19bd23151b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset is  4473648\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the dataset is \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c3547c-d2de-442f-b5f4-65bd532795fe",
   "metadata": {},
   "source": [
    "#### First 1,000 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84581b2b-7b01-46a0-87e1-8df2594a8824",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High up, crowning the grassy summit of a swelling mound whose sides are wooded near the base\n",
      "with the gnarled trees of the primeval forest, stands the old chateau of my ancestors. For centuries\n",
      "its lofty battlements have frowned down upon the wild and rugged countryside about, serving\n",
      "as a home and stronghold for the proud house whose honoured line is older even than the moss-grown\n",
      "castle walls. These ancient turrets, stained by the storms of generations and crumbling under\n",
      "the slow yet mighty pressure of time, formed in the ages of feudalism one of the most dreaded\n",
      "and formidable fortresses in all France. From its machicolated parapets and mounted battlements\n",
      "Barons, Counts, and even Kings had been defied, yet never had its spacious halls resounded to\n",
      "the footsteps of the invader.\n",
      "But since those glorious years all is changed. A poverty but little above the\n",
      "level of dire want, together with a pride of name that forbids its alleviation by the pursuits\n",
      "of commercial life, have prevented\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a75cae0-e844-4254-99f8-8d11c79a8b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset includes the following characters:\n",
      " !#$&'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz| ¡¢£°·½¿ÁÅÆÉÑÓ×áâäåæçèéêëíîïñóôöúüāēœΝΟΠΣΥαγδηικλμνπςτωἀἐἱἶὀὁ–—‘’“”•′″\n",
      "\n",
      "This dataset has 152 unique characters\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "vocab = ''.join(chars) \n",
    "\n",
    "print(f'This dataset includes the following characters:{vocab}\\n')\n",
    "print(f'This dataset has {vocab_size} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e50b3c-9afd-4081-a766-36c715a07198",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07cac667-5b03-417b-904c-f57d70f4b937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3666, 1438, 318, 28926]\n",
      "My name is Ethan\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from chars to integers\n",
    "stoi = {char:i for i,char in enumerate(chars)} #based on index in our sorted array of unique chars, assign number to each character in dictionary (for encoding)\n",
    "itos =  {i:char for i,char in enumerate(chars)} #do the same thing but have index as key and char as value (for decoding)\n",
    "\n",
    "encode = lambda s: enc.encode(s) # given string s, return an array of ints that pertain to each character\n",
    "decode = lambda l: enc.decode(l) # given array of integers, decode into chars using itos and turn into string\n",
    "\n",
    "print(encode(\"My name is Ethan\"))\n",
    "print(decode(encode(\"My name is Ethan\")))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d9456ff-f6d3-44f5-90a8-98a44e9d3bfa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1074176]) torch.int64\n",
      "tensor([11922,   510,    11, 12389,   278,   262,  8701,    88, 14237,   286,\n",
      "          257, 29844, 31446,  3025,  5389,   389,  4898,   276,  1474,   262,\n",
      "         2779,   198,  4480,   262, 19967,   283,   992,  7150,   286,   262,\n",
      "         6994,  2100,  8222,    11,  6296,   262,  1468,   442,   378,   559,\n",
      "          286,   616, 18668,    13,  1114, 10675,   198,   896, 37248,  3344,\n",
      "          902,   423, 32198,   866,  2402,   262,  4295,   290, 30957, 25708,\n",
      "          546,    11,  7351,   198,   292,   257,  1363,   290, 32753,   329,\n",
      "          262,  6613,  2156,  3025, 45870,  1627,   318,  4697,   772,   621,\n",
      "          262, 42853,    12, 22377,   198, 18676,  7714,    13,  2312,  6156,\n",
      "        41104,    11, 34568,   416,   262, 20228,   286, 10439,   290, 37550,\n",
      "          739,   198,  1169,  3105,  1865, 18680,  3833,   286,   640,    11,\n",
      "         7042,   287,   262,  9337,   286, 37591,  1042,   530,   286,   262,\n",
      "          749, 39229,   198,   392, 24599,  6285, 16746,   287,   477,  4881,\n",
      "           13,  3574,   663,  3235, 27045,   515, 41406,  1039,   290, 12623,\n",
      "         3344,   902,   198, 10374,   684,    11,  2764,    82,    11,   290,\n",
      "          772, 10578,   550,   587, 47437,    11,  1865,  1239,   550,   663,\n",
      "        40894, 24350,   581,  6302,   284,   198,  1169, 27146,   286,   262,\n",
      "          800,  5067,    13,   198,  1537,  1201,   883, 21140,   812,   477,\n",
      "          318,  3421,    13,   317,  8098,   475,  1310,  2029,   262,   198,\n",
      "         5715,   286, 19958,   765,    11,  1978,   351,   257, 11293,   286,\n",
      "         1438,   326, 44114,   663, 23863,  3920,   416,   262, 45968,   198,\n",
      "         1659,  5068,  1204,    11,   423, 13351,   262,   629,   507,   286,\n",
      "          674,  1627,   422, 10941,   511, 42252,   287,   198,  1050, 32248,\n",
      "        32394,   454,    26,   290,   262,  7463, 14966,   286,   262,  7714,\n",
      "           11,   262,   625, 22377, 28459,   287,   262, 14860,    11,   198,\n",
      "         1169,  5894,   290, 36972,  6941,   265,    11,   262,  2801,    12,\n",
      "           79,  9586,  1093,   774,  1371,    11,   290, 23126,  1359, 18028,\n",
      "         1231,    11,   355,   880,   355,   262,   198,    82, 16406, 18570,\n",
      "           11,   262, 23849,    12,  4098,   268,   266,  1299,    66,  1747,\n",
      "           11,   290,   262, 24887,  9814,   395,  1678,  1626,    11,   477,\n",
      "         1560,   257, 46400,   198, 29429,   286,  9292,  4490, 23365,    13,\n",
      "         1081,   262,  9337,  3804,    11,   717,   530,    11,   788,  1194,\n",
      "          286,   262,  1440,  1049, 41104,   198, 22474,  1364,   284, 16866,\n",
      "           11,  1566,   379,   938,   475,   257,  2060, 10580, 23707,   262,\n",
      "        21098,  5322, 25321,   286,   198,  1169,  1752, 18680, 33200,   286,\n",
      "          262,  7964,    13,   198,  1026,   373,   287,   530,   286,   262,\n",
      "         5909,   290, 46400, 23204,   286,   428,  5637, 10580,   326,   198,\n",
      "           40,    11,  3738, 42722,    11,   938,   286,   262, 19283,   290,\n",
      "          697, 17539,   955,  4879,   390,   327,  4500,    11,   717,  2497,\n",
      "          262,  1657,   198,  1659,  1110,    11, 37989,   890,   812,  2084,\n",
      "           13, 12511,   777,  7714,    11,   290, 12077,   262,  3223,   290,\n",
      "        40175, 17039,    11,   198,  1169,  4295, 24343,  1127,   290,  7128,\n",
      "          926,  3028,   286,   262, 12788,  1589,  2174,    11,   547,  3377,\n",
      "          262,   717,   812,   286,   616, 17840,   198,  6042,    13,  2011,\n",
      "         3397,   314,  1239,  2993,    13,  2011,  2988,   550,   587,  2923,\n",
      "          379,   262,  2479,   286, 12277,    12, 11545,    11,   257,  1227,\n",
      "          878,   198,    40,   373,  4642,    11,   416,   262,  2121,   286,\n",
      "          257,  7815,  7599, 19621,   375,  2004,   422,   530,   286,   262,\n",
      "        36043, 41406,  1039,   286,   262,   198, 18676,    26,   290,   616,\n",
      "         2802,  1719,  3724,   379,   616,  4082,    11,   616,  1337,   290,\n",
      "         3707,  1614,  5634,  9944,  2402,   530,   198,  2787,  1397,  1113,\n",
      "         2072,    11,   281,  1468,   290, 13467,   582,   286, 11091,  4430,\n",
      "           11,  3025,  1438,   314,  3505,   198,   292, 21204,    13,   314,\n",
      "          373,   281,   691,  1200,    11,   290,   262,  3092,   286, 19429,\n",
      "         1056,   543,   428,  1109,   920,  6255,  2402,   198,  1326,   373,\n",
      "        30259,   416,   262,  6283,  1337, 25805,   416,   616,  9722, 21688,\n",
      "          287, 23494,   502,   422,   262,   198, 35634,  1905,   286,   262,\n",
      "        30779,  1751,  3025,   450,  4147,   547, 16830,   994,   290,   612,\n",
      "         2402,   262, 36149,   326,   198, 11793,   744,   262,  2779,   286,\n",
      "          262, 12788,    13,  1629,   262,   640,    11, 21204,   531,   326,\n",
      "          428, 17504,   373, 10893,  2402,   198,  1326,   780,   616, 15581,\n",
      "         4082,  4624,   502,  2029,  8112,   351,   884,  3339,  1350,   666,\n",
      "         1664,    13,  2735,   314,   760,   198,  5562,   663,  1103,  2134,\n",
      "          373,   284,  1394,   422,   616, 11368,   262, 21696, 19490,   286,\n",
      "          262, 15157, 17328,  2402,   674,  1627,    11,   198,  5562,   547,\n",
      "        37862,  1297,   290,  7842,  1431,   416,   262,  2829, 18285,   563,\n",
      "          355,   484,  3453,   276,   287,  4791,   704, 39271,   198,   259,\n",
      "          262, 19634,   286,   511, 36856,  3285,  9998,    13,   198, 19093,\n",
      "        11557,    11,   290,  8754,  2402,   616,   898,  4133,    11,   314,\n",
      "         3377,   262,  2250,   286,   616,  9963,   198,   259,   279,  3255,\n",
      "          625,   262,  6156,   284,  6880,   326,  5901,   262,  9082,    12,\n",
      "           71, 20227,  5888,   286,   262,   442,   378,   559,    11,   290,\n",
      "          198,   259, 31665,  1231,  4031,   393,  4007,   832,   262, 29079,\n",
      "        46166,   286,   262, 37410,  4898,   326,  8242,   198,  1169,  1735,\n",
      "          286,   262, 12788,  1474,   663,  2366,    13,   632,   373,  3737,\n",
      "          281,  1245,   286,   884, 21334,   326,   616,  2000,   198, 11458,\n",
      "         9477,   257, 17979,   286, 47886,    13,  5845,  3640,   290, 45968,\n",
      "          543, 48513,   286,   262,  3223,   290,   198, 13966,   586,   287,\n",
      "        10362,   749,  7634,  4752,   616,  3241,    13,   198,  5189,   616,\n",
      "          898,  3234,   314,   373, 10431,   284,  2193, 18032,   306,  1310,\n",
      "           11,  1865,   644,  1402,  3725,   198,  1659,   340,   314,   373,\n",
      "         1498,   284,  4461,    11,  3947,   284, 41447,   502,   881,    13,\n",
      "         8673,   340,   373,   379,   717,   691,   262, 10561,   198,   260,\n",
      "         2290,   310,   590,   286,   616,  1468, 40550,   273,   284,  2112,\n",
      "          351,   502,   616, 38082, 29171,   326,  2921,  4485,   284,   262,\n",
      "          198, 14007,   543,   314,  1683,  2936,   379,   262,  3068,   286,\n",
      "          616,  1049,  2156,    26,  1865,   355,   314,  6348,   503,   286,\n",
      "         9963,    11,   314,   198,  9776,  1498,   284,  3704,  1978, 28597,\n",
      "        21441,   286, 18129,    11,  1309, 13819,   422,   262, 19084,   198,\n",
      "           83,   506,   518,   543,   550,  9258,   284, 24215,   353,   287,\n",
      "        13885,  3308,   879,    11,   326,   550,   257,  3297,   286,  8695,\n",
      "          284,   257,  1728,   198, 21170,   388,   301,   590,   543,   314,\n",
      "          550,  1464, 10762,  6283,    11,   475,   543,   783,  2627,  5391,\n",
      "          306,  7818,    13,   383, 25179,   198,  1462,   543,   314,   477,\n",
      "         2507,   318,   262,  1903,  2479,   379,   543,   477,   262,   955,\n",
      "         4879,   286,   616,  1627,   550,  1138,   511,   886,    13, 40661,\n",
      "          198,    40,   550, 45053,  3177,   428,   475,   257,  3288, 11688,\n",
      "          286,   257,  1641,   286,  1790,    12, 24489,  1450,    11,   314,\n",
      "        20875,   198,    79,   623,  1068,   890,  2402,   777, 19905,  7040,\n",
      "           11,   290,  2540,   284,  2018,   606,   351,   262, 27776,   654])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334a0349-e7cf-49b0-8e24-7ed619f3688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50258"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_subwords = torch.unique(data, sorted=True)\n",
    "vocab_size =  50258 # or try 50257\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10aea9d-bff8-48eb-b8c1-813ded0b09ee",
   "metadata": {},
   "source": [
    "## Separate into train and test validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f51ab21-5850-4c33-b7b9-816b30d7baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data)) # 90% train x 10% test split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36152d91-6b5b-4c2a-bb02-810aa88348ac",
   "metadata": {},
   "source": [
    "## Set block size for chunking\n",
    "train in chunks of n tokens at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e19c474-c006-4ef4-b22c-94f39fa5b781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11922,   510,    11, 12389,   278,   262,  8701,    88, 14237,   286,\n",
       "          257, 29844, 31446,  3025,  5389,   389,  4898,   276,  1474,   262,\n",
       "         2779,   198,  4480,   262, 19967,   283,   992,  7150,   286,   262,\n",
       "         6994,  2100,  8222,    11,  6296,   262,  1468,   442,   378,   559,\n",
       "          286,   616, 18668,    13,  1114, 10675,   198,   896, 37248,  3344,\n",
       "          902,   423, 32198,   866,  2402,   262,  4295,   290, 30957, 25708,\n",
       "          546,    11,  7351,   198,   292,   257,  1363,   290, 32753,   329,\n",
       "          262,  6613,  2156,  3025, 45870,  1627,   318,  4697,   772,   621,\n",
       "          262, 42853,    12, 22377,   198, 18676,  7714,    13,  2312,  6156,\n",
       "        41104,    11, 34568,   416,   262, 20228,   286, 10439,   290, 37550,\n",
       "          739,   198,  1169,  3105,  1865, 18680,  3833,   286,   640,    11,\n",
       "         7042,   287,   262,  9337,   286, 37591,  1042,   530,   286,   262,\n",
       "          749, 39229,   198,   392, 24599,  6285, 16746,   287,   477,  4881,\n",
       "           13,  3574,   663,  3235, 27045,   515, 41406,  1039,   290, 12623,\n",
       "         3344,   902,   198, 10374,   684,    11,  2764,    82,    11,   290,\n",
       "          772, 10578,   550,   587, 47437,    11,  1865,  1239,   550,   663,\n",
       "        40894, 24350,   581,  6302,   284,   198,  1169, 27146,   286,   262,\n",
       "          800,  5067,    13,   198,  1537,  1201,   883, 21140,   812,   477,\n",
       "          318,  3421,    13,   317,  8098,   475,  1310,  2029,   262,   198,\n",
       "         5715,   286, 19958,   765,    11,  1978,   351,   257, 11293,   286,\n",
       "         1438,   326, 44114,   663, 23863,  3920,   416,   262, 45968,   198,\n",
       "         1659,  5068,  1204,    11,   423, 13351,   262,   629,   507,   286,\n",
       "          674,  1627,   422, 10941,   511, 42252,   287,   198,  1050, 32248,\n",
       "        32394,   454,    26,   290,   262,  7463, 14966,   286,   262,  7714,\n",
       "           11,   262,   625, 22377, 28459,   287,   262, 14860,    11,   198,\n",
       "         1169,  5894,   290, 36972,  6941,   265,    11])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#down the road we want the transformer to train at each subsequent token\n",
    "#e.g. 18 -> 47, 18 + 47 -> 56, 18 + 47 + 56 -> 58, etc. (see next kernel)\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4525c5b1-2b39-4671-b48b-510dca5c0a60",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([11922]), the target is: 510\n",
      "when input is tensor([11922,   510]), the target is: 11\n",
      "when input is tensor([11922,   510,    11]), the target is: 12389\n",
      "when input is tensor([11922,   510,    11, 12389]), the target is: 278\n",
      "when input is tensor([11922,   510,    11, 12389,   278]), the target is: 262\n",
      "when input is tensor([11922,   510,    11, 12389,   278,   262]), the target is: 8701\n",
      "when input is tensor([11922,   510,    11, 12389,   278,   262,  8701]), the target is: 88\n",
      "when input is tensor([11922,   510,    11, 12389,   278,   262,  8701,    88]), the target is: 14237\n",
      "when input is tensor([11922,   510,    11, 12389,   278,   262,  8701,    88, 14237]), the target is: 286\n",
      "when input is tensor([11922,   510,    11, 12389,   278,   262,  8701,    88, 14237,   286]), the target is: 257\n"
     ]
    }
   ],
   "source": [
    "#training like this helps with computational efficiency but also to help expose the transformer\n",
    "#to more context from 1 - blocksize. Needs to get used to seeing everything in between 1-block size\n",
    "#Should also be noted that transformer will NEVER predict based on series of tokens > blocksize (only from 1-bs)\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(10):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context}, the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21e216-1c01-4aaa-8c34-f888c4221711",
   "metadata": {},
   "source": [
    "## Batch Dimensions\n",
    "Going to have chunks of text encodings stacked up in a single tensor for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1956411-a829-47dd-b393-efb59abcbddc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_size = block_size\n",
    "\n",
    "def get_batch(split: str):\n",
    "    #generate small batch of data of inputs x with targets to predict y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate batch_size-sized random indices to get data from\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # for each randomly selected index, get the associated context (up to block size) in the data\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # get offset by so x can try to predict it\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad() #don't update on backwards - more memory efficient\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() #effectively average across many iterations of our training to smooth out loss\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a07d6c34-4259-4af0-94a9-47b92821ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19d26347-5029-49eb-a4df-2215bc2878e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49958, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(test_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cc06a0c-b3b3-4fc8-bac4-95695e430cf8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([16, 256])\n",
      "tensor([[ 6427,   286,   734,  ...,   251,   198,   464],\n",
      "        [   13,    50,    13,  ...,   287,   262,  3329],\n",
      "        [  616,  3488,   413,  ...,  1625,  1969,   284],\n",
      "        ...,\n",
      "        [29843, 24260, 16537,  ..., 32606,   547,   757],\n",
      "        [10647,   287, 20302,  ..., 16574,   198,   259],\n",
      "        [26280,  9538,   326,  ...,  8350,   262, 44395]], device='cuda:0')\n",
      "targets: torch.Size([16, 256])\n",
      "tensor([[  286,   734, 15953,  ...,   198,   464,  9813],\n",
      "        [   50,    13,   284,  ...,   262,  3329,   198],\n",
      "        [ 3488,   413,   446,  ...,  1969,   284,  9336],\n",
      "        ...,\n",
      "        [24260, 16537,  6190,  ...,   547,   757, 17700],\n",
      "        [  287, 20302,    11,  ...,   198,   259,   257],\n",
      "        [ 9538,   326,   314,  ...,   262, 44395,   286]], device='cuda:0') \n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = get_batch('train')\n",
    "print('inputs:', x_batch.shape)\n",
    "print(x_batch)\n",
    "\n",
    "print('targets:', y_batch.shape)\n",
    "print(y_batch, \"\\n\\n----------\\n\")\n",
    "\n",
    "# for batch in range(batch_size):\n",
    "#     for time in range(block_size):\n",
    "#         context = x_batch[batch,:time+1]\n",
    "#         target = y_batch[batch,time]\n",
    "#         print(f'When input is {context}, target is {target}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52c815d7-634b-4d9e-8336-4e05f22d8f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50258"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50933ded-538c-4ef4-9d03-166a6ba37584",
   "metadata": {},
   "source": [
    "## Define Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ccc555c-3b7e-43fd-93af-7a44682250b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #creates the triangular matrix of 1s that makes our mask\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # B,T,16 (head size)\n",
    "        q = self.query(x) # B,T,16 (head size)\n",
    "        # Now we have B, T, 16 tensors where B and T are in parallel - no communication\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # B,T,16 @ B,16,T ----> B,T,T - Weighted aggregation now is a function of the keys and queries of these \n",
    "                                                # Scaled attention to preserve variance\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # B,T,T\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x) # what gets aggregated for the purpose of this single head\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList((Head(head_size) for _ in range(num_heads)))\n",
    "        self.projection = nn.Linear(n_embd, n_embd) #linear transformation of the outcome of forward\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) #concatenate outputs for as many heads as we want working in parallel\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self,n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # projection layer going back into residual pathway\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x) #makes it so that all tokens experience the activation functions independently\n",
    "                           #self attention is the communication, and this feedforward is the opportunity for individual tokens to think on what they learned\n",
    "    \n",
    "\n",
    "class Block(nn.Module): \n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    #intersperses comm. and comp.\n",
    "    #communication done by the multiheaded self-attention and the computation done by the feedforward network on all tokens independently\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # n_head heads of head_size-dimensional self-attention\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embd)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #just adding blocks by themselves increases complexity / how deep the network is with basically the same results. Need to work on the optimization issues\n",
    "        x = x + self.sa(self.layernorm1(x)) # 'x = x + 'allows for residual connections while layernorm 1 and 2 allow for normalized rows before self-attention and feedforward\n",
    "        x = x + self.ffwd(self.layernorm2(x))\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "class BigramLanguageModel(nn.Module): #subclass of nn Module\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__() #call parent's constructor\n",
    "        # each token directly reads off the logits for the next token from lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # create vocab_size x vocab_size embedding table\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])   #intersperse communication and computation 3 times with the 3 blocks\n",
    "        self.layernorm_final = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        # print(torch.max(index))\n",
    "        #index and targets are both (B,T) tensor of integers\n",
    "        #when we pass an index here, every integer in our input will refer to the embedding table\n",
    "        #and pluck out a corresponding row in the table according to its index\n",
    "        #e.g. when we hand it 25 (the encoding of 'M'), it goes to row 25 in embedding table\n",
    "        #then pytorch will arrange it into a Batch x Time x Channel tensor \n",
    "        # Hence, our logits end up as being 4 (batch_size for parallel processing) by 8 (# of context places / block_size) by 65 (vocab_size)\n",
    "        #(B,T,C). Remember, logits are just log counts of a distribution\n",
    "        #logits are our scores for the next character in the sequence\n",
    "            \n",
    "            \n",
    "        tok_emb = self.token_embedding_table(index) #B,T,C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T,device=device)) #T,C\n",
    "        x = tok_emb + pos_emb # B,T,C\n",
    "        x = self.blocks(x) #apply one head of self attention B,T,C\n",
    "        x = self.layernorm_final(x)\n",
    "        logits = self.lm_head(x) #B,T,vocab_size\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None #because there's nothing to aim for\n",
    "        \n",
    "        else:\n",
    "            #torch gets angry if we give it C in the third dimension so we need to reshape\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # reshape into a 32 x 65 tensor\n",
    "            targets = targets.view(B*T)\n",
    "            #need to evaluate the loss function\n",
    "            # let's use negative log likelihood / crossentropy\n",
    "            loss = F.cross_entropy(logits,targets) # how well are we predicting next character based on the logits?\n",
    "                                                   # ideally, the correct dimension (point at 4,2,45 for example) should have a very high number while others are low\n",
    "        return logits, loss\n",
    "\n",
    "    #Essentially a 'predict' function that 'generates' new \n",
    "     #index is (Batchsize,Time) tensor of integers in current context\n",
    "                                                    #max_new_tokens is max number of tokens to generate (?)\n",
    "    def generate(self, index, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            #crop index to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:] # helps with the positional encoding - don't feed info we don't have access to\n",
    "            \n",
    "            #get predictions\n",
    "            logits, loss = self(index_cond) # calls forward\n",
    "            #focus on only last time step (want the most trained version of the model)\n",
    "            logits = logits[:, -1, :] \n",
    "            #apply softmax to get probabilities (exponentiate to approximate counts then get proportions to approximate probabilities)\n",
    "            probs = F.softmax(logits, dim=-1) # B,C\n",
    "            #sample from the distribution to get next character\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # B, 1, in each batch dimension we have a single prediction of what comes next\n",
    "            #append sampled index to running sequence given current context of what we've predicted before\n",
    "            index = torch.cat((index,index_next),dim=1) # B, T+1\n",
    "        return index\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85161d3a-a717-4dd2-805f-eb7fa1cc06b0",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a0beac6-7781-435c-92f7-7ab005df5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e8a5c40-dcf7-4c96-937c-53bc7ba9a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(decode(m.generate(index = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=500)[0].tolist()))\n",
    "#Gives us garbage because our current model is only looking at the last character bc it's a bigram model\n",
    "#Also not trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99f63b-66a6-4a03-88ad-b248cc23e31b",
   "metadata": {},
   "source": [
    "## Let's Optimize and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50ae5c41-7b61-4b0a-8c5d-a37f65d64a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edca7b79-3185-44aa-befd-e1ee417766fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 11.0176, val loss 11.0179\n",
      "step 500: train loss 5.7377, val loss 6.5235\n",
      "step 1000: train loss 5.1687, val loss 6.3116\n",
      "step 1500: train loss 4.8515, val loss 6.2283\n",
      "step 2000: train loss 4.5734, val loss 6.2014\n",
      "step 2500: train loss 4.3331, val loss 6.2097\n",
      "4.429532527923584\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for steps in range(max_iters):\n",
    "     # every once in a while evaluate the loss on train and val sets\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        # janky earlystopping\n",
    "        if losses['val'] - losses['train'] > 1.8:\n",
    "            break\n",
    "    #sample batch of data\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    #evaluate loss\n",
    "    logits, loss = m(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b65c46b9-89fa-46c1-8227-24596d5b26eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!”! BUZZ.\n",
      "“ya sleeping?” tw than! If—Still and record\n",
      "But I puzzled me no more and mumbling. The beginnings and horror seemed as the next sentence\n",
      "unseen ancient letter to assume you slowly followed him, and soon bought it in fear with the sacred\n",
      "Sable fortress.”\n",
      "The natural features, we continued, and had no chance to Yuanurined\n",
      "and for the glorious outlines of a secondtheir sensation. Seeing perhaps be tongues, then suddenly jumped Said! A taxi’svenge\n",
      "controllerillogical (adows here is still Tanner”!\n",
      "“A snakes that time of healing be the notes were, the Membersion of my consciousness name—speakable\n",
      "memories first eyes! After this?”\n",
      "Since Walter first sprawled by a wave-li-dead some of a clumsy—and unlit amounts! Howments\n",
      "“What you mean to this reserve! Orabific Peck from the things\n",
      "Opening at least sounded? Hal? Hey? M”\n",
      "Albert we all the record scroll said, came I shuddered\n",
      "itself, creepinging solicitously likeify me and speeches, and at the thing were the thing settled on\n",
      "abters. Certainly, this time would provide\n",
      "come and less grave. It was the course of that I learned these conditions around? Is it fastorous\n",
      "little in its excitement! Or seemed to notice those above. The nearest\n",
      "ill-flere z medic, and their men came upon the realities.\n",
      "“I suppose it was worshipped 1880 that .”\n",
      " Prophet had old fears at many melody, McTig’s\n",
      "more than that I was trying to recognise the lines. When I am built and let him, and\n",
      "be differently waters I’ve had fer the creature again. Why I’ll believe it to me that look at\n",
      "rats from was centred. There was a breathless as morbid strain of course\n",
      "his, but just as soon as possible. If I have known at all sixened\n",
      "their proper will. . . . it is more than beneath the front sticky stuff and nameless three sun. . . I\n",
      "me morning the last forty-seen and feebeless monsters of lightning stops; an instant\n",
      "but I heard a wharf of sash-grandfather from dust. The vision is queerly connected with the smell\n",
      "along me, and you heard of it alive and must be to concealment.\n",
      "It is old, but that is not be more bad significance. One is traces, and you have known Wisdom.\n",
      "horror of the old link! To your city and my presence which lies dreamage Wious and won’t\n",
      "they. It amounts, spasmal restoration are little of reverence for me, but following me always tonight\n",
      "to laugh. I have how secret sense, and I looks now stepped and gone mad a tree\n",
      "that very seem disconnected and on the ground repeats over,\n",
      "and along the mate last there. So, I have been\n",
      "to study of us in a long, but not doubt for protection? It are not what real causes that they are directed? Was the first’s it looks\n",
      " ye de garuck now? Why, Dan is the beings of good dogs and pleasing Aira Bickirickes\n",
      "that well beat 1844—pushing Beren drop Pickman’s hunters—I\n",
      "men that jointer bluntly isried, perhaps have doubtless an incredible than they were\n",
      "indeed. Weeden, for us suggest the secrets\n",
      "servants seemed those scattering and fear of them off in their ways; though the excitement,\n",
      "was still alive latent with that inexplicable, a discovery whom no man had seen an aura in Oukran 1924.\n",
      "“You he comes to work about thy young devils you, and that came to yourimilt, so that\n",
      "more stir, and I am Hoth, and here looking.\n",
      "They recall that only the blue and your Surama in his nobles while the\n",
      "st classical poor troubled him, and once to persons in the crystal table down to\n",
      "speaking on Trouble. But still there is a thing before is now in the wrong\n",
      "house, old Carter saw that his company. But when she was path, and a\n",
      "more rose a time a stake of transmit entrusted morning fields and the autumn of Professor Uth city\n",
      "was superb ashore, and the gooding to man on his fragments till the dreamerof cosmic telephone\n",
      "bungations, was spreading.\n",
      "One day the immediate of the skies never ofalking assigned in all dances forever of Africa; making staring\n",
      "ficialetrical-and ran as long before he is regimen. Only all gone out. Audrey I came the eas staff,\n",
      "plumbed underthemselves, and he saw the ground while the journey changed and approached the year if of\n",
      "dragans were scenes are being due and solemnly through the endless waters. The dreams of earth’s room,\n",
      "there I was safe in the hole of sod betwixt the earth tree. All the moon,\n",
      "its terrible thing during the void and unbroken cities and floated around me,\n",
      "with one who breathed to cast from newspaper and lowering along the gardens, as low wealth from which no less cotuporahs.\n",
      "I shivered, the blossed away on the streams of a sticky joke. What\n",
      "where the town was passed by that hideous rats one thing, while the feelings of making myDoesolts\n",
      "of a period in Arkham told me lead conclusions the roof current—who had paid up and his\n",
      "vestught, and time since whatever lower eyes I left me in heavy cosmic triumph, and approached the fire of\n",
      "intangible a the government a non leading accomplishments and swollen dizzily that Milton. It was the signs of\n",
      "eons ago, and a trap-bon.\n",
      "“So you lived in God,” he dropped home and upon\n",
      "early with tribes on the plain, and all over the way—on the room applies a trifles of them\n",
      "mercharm and crumbling with singular embodiment hut as everything laid down, communication left in\n",
      "squamous. He said she went back it lie upon the hills University, and indeed, and only 72\n",
      "him at times on the table and August 12, and squat briskthings received jocued trib-t04’s of mottings and\n",
      "Telaze in the first emptiness. The villagers always accurate as\n",
      "they told me, and that the pallid murders on several professors, light of aromatic regions.\n",
      "It was no more than two men who wore their adversariesing from the floor,\n",
      "so not to keep the left to such a row of tragedy; for\n",
      "the fleeing stones at the more attacks found shores of the deep outside time amongst several\n",
      "which to the earth at the bondage of night. All of that Old Ones are at sunset, there\n",
      "are pen no spawn, or people returned to shewingoured and the stars\n",
      "primal frescoach on the ways where the fairy-heash-e were discussing, and the\n",
      "plan in languages.\n",
      "In which is all other regular Arthur Jermyn-group and pulling the prehistoric elder senses was growing by some cult whose sort\n",
      "crest in moderately, power out of of their own on his Bay,\n",
      "and against the high windows and a cloud far water guarding houses and likeness the old\n",
      "me out of the Pe DOOM them.\n",
      "But no mist-footed of Kehd toward the limbs, there appeared, and he played each of\n",
      "found wrapping. Before first to\n",
      "Lows Falls, while this hideous forestalling beneath the local\n",
      "les, immemiss, and there was strange in order that typical 1strowned\n",
      "door in the will which seemed report the blank Azathog’s mentality, there soon drifted efear\n",
      "poters of small priests. So from the other side he built with celestial—a\n",
      "Ought of sunless heights they found a gorgeous diaboler–ized associations—bold in\n",
      "car) and regular peacured hand, and heeded the scmarked or sometimes it. The spirit shewed\n",
      "the men between the chel! The tissue party of SpiritWallet worth, securing the Great Old Ones, giving theussy and\n",
      "had become after not seen many of sea, but West he is generally, became a obnoxious\n",
      "and blear on fainted and dead. This was he motion, first uncurl, a straight, and demanding from education and 55 tautiously dreaded\n",
      "like the roofs of the Wichita phantakward electric whose a unexpected trees normal\n",
      "de at unimaginable of great revelation in bedconvor. His breeds and light glibbered at hand was only\n",
      "forth seen in Arkham. He was only he had never allowed his presence at Frankian\n",
      "his gloves; but when he was finishing that he said that he strode questioned him in this pillars.\n",
      "Very easy to him snorted Mr. Ward brought to himself, but it, it is in 16problems of the first\n",
      "old history in China; producing as they like one date and healthy\n",
      "on which they set him around Whateleys and about those odd\n",
      "on lay in the rocky towers of the South Street and found on through the dusty bank,\n",
      "but one wherein by a Trib avail eyes picked up to a crypt than he had said the common case. Of Whateley\n",
      "infinityin the acts, he felt the naturally sent to his wife, but had come\n",
      "his son pubour which quarrel, almost simultaneously at being ever their brother in a gold ransom, and communicating\n",
      "the no light insolets reassuring of it warm fidelity. His\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(index = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a245fec5-c53f-44ad-919d-7b5aae921844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "torch.save(m.state_dict(), 'gpt-model-lovecraft-subword.pth')\n",
    "torch.save(m.state_dict(), 'gpt-model-state-dict-lovecraft-subword.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f544c-78eb-4ffb-902c-b99eb69c8d6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mathematical Trick in Self-Attention - 3 Methods\n",
    "How Transformers differ from traditional LSTM / RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675804fa-38d9-4579-be32-088aa28d4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 #batch, time, channels\n",
    "#Review Batch is for parallel processing, time is the amount of context, channels are the characters that can be predicted\n",
    "\n",
    "#For predicting, information should only flow from previous context to current. It should not take from future tokens in training (e.g. In the word 'Hamburger,'\n",
    "#it shouldn't use 'u' as influence for choosing 'b')\n",
    "\n",
    "#The easiest way for tokens to communicate is to do an average of all the preceding elements\n",
    "#You can then add that average of previous context as a feature vector what you already know\n",
    "#Recognize that an average is a very weak/lossy way of summarizing info, but the principle of summarizing what you already know into\n",
    "#a number that represents previous context is key\n",
    "x = torch.randn(B,T,C) #fill 4,8,2 tensor with random numbers\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265f90f-4973-4927-85b1-1b2e941f1d22",
   "metadata": {},
   "source": [
    "### Self-Attention Mask 1 - Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99da2a-4a2e-42fe-949e-716079bf65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[batch, time] = mean_{i<=t} x[b,i]\n",
    "x_bag_of_words = torch.zeros((B,T,C))\n",
    "for b in range(B): #iterate over batch dimensions\n",
    "    for t in range(T): #iterate over time\n",
    "        x_prev = x[b,:t+1] #(t,C)\n",
    "        x_bag_of_words[b,t] = torch.mean(x_prev, 0) #averaging out previous x's over time\n",
    "\n",
    "print(x[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80468757-367d-4753-bd96-eb18c75d4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_bag_of_words[0])\n",
    "#each element is an average of prev elements in tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fbfe0b-104f-4c46-9da3-125dadbca7c4",
   "metadata": {},
   "source": [
    "### Self-Attention Mask 2 - Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0dad0-5064-4a4b-a315-cc18600489fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication basics\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "print(\"Works by multiplying first row of a by first col of b and adding up (1*2 + 1*6 + 1*6 = 14), same thing for 16 (7+4+5) etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15292d71-fe22-4995-af7a-457bd62433e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3)) # gives lower triangular part of matrix\n",
    "#as you progress down the matrix, you progressively ignore 1 less element of b due to the growing # of 1s and shrinking number of 0s\n",
    "a /= torch.sum(a, 1, keepdim=True)\n",
    "#now we are able to average the sums going down rows because for each row we are essentially multiplying it by 1/row_num (which is an average)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182ea26-a08b-4db0-967f-a60297b83ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights / weights.sum(1, keepdim = True)\n",
    "x_bag_of_words_2 = weights @ x # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "# xbow and xbow2 should be equal but this way is MUCH faster\n",
    "# essentially we are doing weighted sums by using the triangular torch.tril so that we can only have the matrix access\n",
    "# only the tokens preceding it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29369b51-5a9e-4cb1-a5fb-019a1419f5b3",
   "metadata": {},
   "source": [
    "### Self-Attention Mask 3 - Using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9821b-d4a4-4913-aca7-d220999c1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #fill zeros with -inf\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and average across each row -> end up with same matrix as the previous two methods\n",
    "x_bag_of_words_3 = wei @ x\n",
    "x_bag_of_words_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e53274-a2f9-48f1-9ed9-4ce9aa338743",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Self-Attention Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e5898-fe83-4104-8a56-de2690584a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn((B,T,C))\n",
    "\n",
    "#emit a query and a key vector\n",
    "#Query vector - what am I looking for\n",
    "#Key vector - what do i contain\n",
    "#obtain affinities between vectors by effectively doing a dot product between the keys and the queries\n",
    "# e.g. A query dot products with all the keys and that dot product becomes wei\n",
    "\n",
    "#Single head that performs self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # B,T,16 (head size)\n",
    "q = query(x) # B,T,16 (head size)\n",
    "# Now we have B, T, 16 tensors where B and T are in parallel - no communication\n",
    "wei = q @ k.transpose(-2, -1) # B,T,16 @ B,16,T ----> B,T,T - Weighted aggregation now is a function of the keys and queries of these nodes\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T)) # don't want it to be all uniform because some tokens will have natural affinities for or against other tokens in the past (this currently doesn't do that)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x) # what gets aggregated for the purpose of this single head\n",
    "out = wei @ v \n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904f22d-ba9e-48a2-973c-8bd1d159693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei[0] #now wei is data dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c043f5-34fe-4547-881f-e42938fe98ed",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Attention is a **communication mechanism.** Can be seen as nodes in a directed graph looking at each other and aggregating into a single value with a weighted sum from all nodes that point to them with data-dependent weights \n",
    "- There is no notion of space\n",
    "- There is no communication across batch dimensions\n",
    "- Attention block doesn't necessarily have to only communicate backwards (E.g. Token at Pos 5 will never look forward to Pos 6 to choose what token it should be). In some cases you may want to have all tokens talk to each other (e.g. sentiment analysis where future context words may shed a different light on previous ones). To create such an encoder block, all you have to do is delete wei = wei.masked_fill(tril == 0, float('-inf')), because that removes the mask of -inf and allows for aggregates at all Ts regardless of temporal position\n",
    "    - What we have currently is called a decoder block\n",
    "- There is also something called cross-attention. This block of code is called self-attention because the keys, queries, and values are all coming from the same source (x). However, there can be a case where your queries come from x but key and values come from another source. \n",
    "    - Called cross attention if there is a separate pool of nodes that we want to pool information from \n",
    "- Scaled self attention additionally divides wei by 1/sqrt(head_size) - this makes it so when input Query, Key are dot producted, the resulting variance will be preserved to approximately 1 as opposed to the order of head_size\n",
    "    - Prevents softmax from being way too peaky because with multiplication it tends to optimize towards the max values, leading to high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723b153-f96b-4478-9107-4809f704124e",
   "metadata": {},
   "source": [
    "# Layernorm Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99bbff-5c98-49a7-a4e8-3b7a6eba4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "    \"\"\" normalize rows to 1 stdev\"\"\"\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "  \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e61f74c-3511-4521-af7d-9e8d4ccd7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,:].mean(), x[0,:].std() # the ROWS are normalized to stdev of 1 and mean of 0 (normal distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399edc0-a651-45fd-a9b2-d30220efcf6b",
   "metadata": {},
   "source": [
    "# Generate Infinite Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2d4a9-f947-4889-8cc3-3f0990dab496",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load('gpt-model-shakespeare.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0c1d9-f775-412c-992f-9de6a0066c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = decode(model2.generate(index = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=2000)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e6aef-814c-4c68-93b5-261bcd30fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare_output.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(model_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
