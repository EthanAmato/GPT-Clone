{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a17808b-7dcd-41bd-8ae3-43ba3643f4ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5563fc71-0678-47e0-8041-944eb168e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90f90ac-723c-44fc-9081-86d2cd482b65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch # lets use pytorch\n",
    "import tiktoken \n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b27f00-6021-466a-ae56-f160c9c3139b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f48a9e-21b0-4925-96ac-7087c9278e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "\n",
    "#384 // 6 = 64 dimensional as standard for each head\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d789a-97e6-479e-af24-52e83d6d4058",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be8dec6c-b6f5-4e22-93ea-95eb5c48dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "486a35a8-e44c-44f3-a5fd-19bd23151b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset is  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the dataset is \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c3547c-d2de-442f-b5f4-65bd532795fe",
   "metadata": {},
   "source": [
    "#### First 1,000 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84581b2b-7b01-46a0-87e1-8df2594a8824",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a75cae0-e844-4254-99f8-8d11c79a8b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset includes the following characters:\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "This dataset has 65 unique characters\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "vocab = ''.join(chars) \n",
    "\n",
    "print(f'This dataset includes the following characters:{vocab}\\n')\n",
    "print(f'This dataset has {vocab_size} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e50b3c-9afd-4081-a766-36c715a07198",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07cac667-5b03-417b-904c-f57d70f4b937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3666, 1438, 318, 28926]\n",
      "My name is Ethan\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from chars to integers\n",
    "stoi = {char:i for i,char in enumerate(chars)} #based on index in our sorted array of unique chars, assign number to each character in dictionary (for encoding)\n",
    "itos =  {i:char for i,char in enumerate(chars)} #do the same thing but have index as key and char as value (for decoding)\n",
    "\n",
    "encode = lambda s: enc.encode(s) # given string s, return an array of ints that pertain to each character\n",
    "decode = lambda l: enc.decode(l) # given array of integers, decode into chars using itos and turn into string\n",
    "\n",
    "print(encode(\"My name is Ethan\"))\n",
    "print(decode(encode(\"My name is Ethan\")))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d9456ff-f6d3-44f5-90a8-98a44e9d3bfa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([338025]) torch.int64\n",
      "tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
      "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
      "          461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
      "         1639,   389,   477, 12939,  2138,   284,  4656,   621,   284,  1145,\n",
      "          680,    30,   198,   198,  3237,    25,   198,  4965,  5634,    13,\n",
      "        12939,    13,   198,   198,  5962, 22307,    25,   198,  5962,    11,\n",
      "          345,   760,   327,  1872,   385,  1526, 28599,   318,  4039,  4472,\n",
      "          284,   262,   661,    13,   198,   198,  3237,    25,   198,  1135,\n",
      "          760,   470,    11,   356,   760,   470,    13,   198,   198,  5962,\n",
      "        22307,    25,   198,  5756,   514,  1494,   683,    11,   290,   356,\n",
      "         1183,   423, 11676,   379,   674,   898,  2756,    13,   198,  3792,\n",
      "          470,   257, 15593,    30,   198,   198,  3237,    25,   198,  2949,\n",
      "          517,  3375,   319,   470,    26,  1309,   340,   307,  1760,    25,\n",
      "         1497,    11,  1497,     0,   198,   198, 12211, 22307,    25,   198,\n",
      "         3198,  1573,    11,   922,  4290,    13,   198,   198,  5962, 22307,\n",
      "           25,   198,  1135,   389, 17830,  3595,  4290,    11,   262,  1458,\n",
      "         1173,  1547,   922,    13,   198,  2061,  4934,   969,  5036,   896,\n",
      "          319,   561, 26958,   514,    25,   611,   484,   198, 19188,  7800,\n",
      "          514,   475,   262, 48713,   414,    11,   981,   340,   547,   198,\n",
      "         1929,  4316,   462,    11,   356,  1244,  4724,   484, 22598,   514,\n",
      "        31533,   306,    26,   198,  4360,   484,   892,   356,   389,  1165,\n",
      "        13674,    25,   262, 10904,  1108,   326,   198,  2001, 42267,   514,\n",
      "           11,   262,  2134,   286,   674, 24672,    11,   318,   355,   281,\n",
      "          198, 24807,   284,  1948,   786,   511, 20038,    26,   674,   198,\n",
      "           82, 13712,   590,   318,   257,  4461,   284,   606,  3914,   514,\n",
      "        15827,   428,   351,   198,   454,   279,  7938,    11,   304,   260,\n",
      "          356,  1716,   374,  1124,    25,   329,   262, 11858,   760,   314,\n",
      "          198, 47350,   428,   287, 16460,   329,  8509,    11,   407,   287,\n",
      "        24613,   329, 15827,    13,   198,   198, 12211, 22307,    25,   198,\n",
      "        17353,   345,  5120,  2592,  1028,   327,  1872,   385,  1526, 28599,\n",
      "           30,   198,   198,  3237,    25,   198, 39276,   683,   717,    25,\n",
      "          339,   338,   257,   845,  3290,   284,   262,  2219,  6017,    13,\n",
      "          198,   198, 12211, 22307,    25,   198, 19626,   345,   644,  2594,\n",
      "          339,   468,  1760,   329,   465,  1499,    30,   198,   198,  5962,\n",
      "        22307,    25,   198, 16371,   880,    26,   290,   714,   307,  2695,\n",
      "          284,  1577,   683,   922,   198, 13116,  6285,    11,   475,   326,\n",
      "          339, 13831,  2241,   351,   852,  6613,    13,   198,   198, 12211,\n",
      "        22307,    25,   198,    45,   323,    11,   475,  2740,   407, 17412,\n",
      "          306,    13,   198,   198,  5962, 22307,    25,   198,    40,   910,\n",
      "        12722,   345,    11,   644,   339, 22027,  1760, 20524,    11,   339,\n",
      "          750,   198,   270,   284,   326,   886,    25,   996,  2705,    12,\n",
      "         5936,   979,  5864,  1450,   460,   307,   198, 11299,   284,   910,\n",
      "          340,   373,   329,   465,  1499,   339,   750,   340,   284,   198,\n",
      "        29688,   465,  2802,   290,   284,   307, 11476,  6613,    26,   543,\n",
      "          339,   198,   271,    11,   772, 10597,   262, 20334,   286,   465,\n",
      "        14675,    13,   198,   198, 12211, 22307,    25,   198,  2061,   339,\n",
      "         2314,  1037,   287,   465,  3450,    11,   345,  1848,   257,   198,\n",
      "        28281,   287,   683,    13,   921,  1276,   287,   645,   835,   910,\n",
      "          339,   318, 25746,    83,   516,    13,   198,   198,  5962, 22307,\n",
      "           25,   198,  1532,   314,  1276,   407,    11,   314,   761,   407,\n",
      "          307, 39497,   286, 14227,    26,   198,   258, 22027, 31025,    11,\n",
      "          351, 18201,    11,   284, 15867,   287, 29693,    13,   198,  2061,\n",
      "        34757,   389,   777,    30,   383,   584,  1735,   267,     6,   262,\n",
      "         1748,   198,   271, 17450,    25,  1521,  2652,   356,   778,   803,\n",
      "          994,    30,   284,   262, 13241,     0,   198,   198,  3237,    25,\n",
      "          198, 16773,    11,  1282,    13,   198,   198,  5962, 22307,    25,\n",
      "          198, 18380,     0,   508,  2058,   994,    30,   198,   198, 12211,\n",
      "        22307,    25,   198,    54, 18906,  6065,   268,  3754,  2449, 14602,\n",
      "           64,    26,   530,   326, 22027,  1464,  6151,   198,  1169,   661,\n",
      "           13,   198,   198,  5962, 22307,    25,   198,  1544,   338,   530,\n",
      "         5508,  1576,    25,   561,   477,   262,  1334,   547,   523,     0,\n",
      "          198,   198, 49275,  1677,    40,  2937,    25,   198,  2061,   670,\n",
      "          338,    11,   616,  1499,  3653,    11,   287,  1021,    30,   810,\n",
      "          467,   345,   198,  3152, 19553,   290,  9784,    30,   383,  2300,\n",
      "           30,  2740,    11,   314, 12472,   345,    13,   198,   198,  5962,\n",
      "        22307,    25,   198,  5122,  1597,   318,   407,  6439,   284,   262,\n",
      "        34548,    26,   484,   423,   198, 18108, 16882,  1359,   428, 46327,\n",
      "          644,   356, 14765,   284,   466,    11,   198,  4758,   783,   356,\n",
      "         1183,   905,   705,   368,   287, 23777,    13,  1119,   910,  3595,\n",
      "          198,  6063,   669,   423,  1913, 45576,    25,   484,  2236,   760,\n",
      "          356,   198, 14150,  1913,  5101,  1165,    13,   198,   198, 49275,\n",
      "         1677,    40,  2937,    25,   198,  5195,    11, 18159,    11,   616,\n",
      "          922,  2460,    11,  6164,  5508, 23788,    11,   198,  8743,   345,\n",
      "        23981, 27012,    30,   198,   198,  5962, 22307,    25,   198,  1135,\n",
      "         2314,    11, 15967,    11,   356,   389, 45171,  1541,    13,   198,\n",
      "          198, 49275,  1677,    40,  2937,    25,   198,    40,  1560,   345,\n",
      "           11,  2460,    11,   749, 21803,  1337,   198, 11980,   262,  1458,\n",
      "         1173,  1547,   286,   345,    13,  1114,   534,  3382,    11,   198,\n",
      "         7120,  7195,   287,   428,   390, 11999,    11,   345,   743,   355,\n",
      "          880,   198, 31584,   379,   262,  9538,   351,   534,   336,  3080,\n",
      "          355, 10303,   606,   198, 39276,   262,  7993,  1181,    11,  3025,\n",
      "         1781,   481,   319,   198,   464,   835,   340,  2753,    11, 25407,\n",
      "         3478,  7319,  1090,  1443,   198,  5189,   517,  1913,  2792,   355,\n",
      "         4625,   621,   460,  1683,   198,  4677,   451,   287,   534, 26795,\n",
      "         3681,    13,  1114,   262,   390, 11999,    11,   198,   464, 11858,\n",
      "           11,   407,   262,  1458,  1173,  1547,    11,   787,   340,    11,\n",
      "          290,   198,  7120, 14475,   284,   606,    11,   407,  5101,    11,\n",
      "         1276,  1037,    13,   978,   441,    11,   198,  1639,   389, 18665,\n",
      "          416, 35765,   414,   198,   817,  1555,   810,   517, 32743,   345,\n",
      "           11,   290,   345, 47397,   198,   464,   932,   907,   267,     6,\n",
      "          262,  1181,    11,   508,  1337,   329,   345,   588, 17150,    11,\n",
      "          198,  2215,   345, 17328,   606,   355,  5775,    13,   198,   198,\n",
      "         5962, 22307,    25,   198, 17784,   329,   514,     0,  6407,    11,\n",
      "         5600,     0,  1119,   497,     6,   263, 19951,   329,   514,   198,\n",
      "        25907,    25,  8659,   514,   284,  1145,   680,    11,   290,   511,\n",
      "         3650,    12, 20089,   198,    66,   859,  1150,   351, 13020,    26,\n",
      "          787,  1225, 14137,   329,   514,  1601,    11,   284,   198, 11284,\n",
      "          514, 17496,    26, 14634,  4445,   597, 17950,   462,   719,   198,\n",
      "        27718,  1028,   262,  5527,    11,   290,  2148,   517,   198,    79,\n",
      "          959,  2259, 24895,  4445,    11,   284,  6333,   510,   290, 39300])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334a0349-e7cf-49b0-8e24-7ed619f3688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50258"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_subwords = torch.unique(data, sorted=True)\n",
    "vocab_size =  50258 # or try 50257\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10aea9d-bff8-48eb-b8c1-813ded0b09ee",
   "metadata": {},
   "source": [
    "## Separate into train and test validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f51ab21-5850-4c33-b7b9-816b30d7baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data)) # 90% train x 10% test split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36152d91-6b5b-4c2a-bb02-810aa88348ac",
   "metadata": {},
   "source": [
    "## Set block size for chunking\n",
    "train in chunks of n tokens at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e19c474-c006-4ef4-b22c-94f39fa5b781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
       "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
       "          461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
       "         1639,   389,   477, 12939,  2138,   284,  4656,   621,   284,  1145,\n",
       "          680,    30,   198,   198,  3237,    25,   198,  4965,  5634,    13,\n",
       "        12939,    13,   198,   198,  5962, 22307,    25,   198,  5962,    11,\n",
       "          345,   760,   327,  1872,   385,  1526, 28599,   318,  4039,  4472,\n",
       "          284,   262,   661,    13,   198,   198,  3237,    25,   198,  1135,\n",
       "          760,   470,    11,   356,   760,   470,    13,   198,   198,  5962,\n",
       "        22307,    25,   198,  5756,   514,  1494,   683,    11,   290,   356,\n",
       "         1183,   423, 11676,   379,   674,   898,  2756,    13,   198,  3792,\n",
       "          470,   257, 15593,    30,   198,   198,  3237,    25,   198,  2949,\n",
       "          517,  3375,   319,   470,    26,  1309,   340,   307,  1760,    25,\n",
       "         1497,    11,  1497,     0,   198,   198, 12211, 22307,    25,   198,\n",
       "         3198,  1573,    11,   922,  4290,    13,   198,   198,  5962, 22307,\n",
       "           25,   198,  1135,   389, 17830,  3595,  4290,    11,   262,  1458,\n",
       "         1173,  1547,   922,    13,   198,  2061,  4934,   969,  5036,   896,\n",
       "          319,   561, 26958,   514,    25,   611,   484,   198, 19188,  7800,\n",
       "          514,   475,   262, 48713,   414,    11,   981,   340,   547,   198,\n",
       "         1929,  4316,   462,    11,   356,  1244,  4724,   484, 22598,   514,\n",
       "        31533,   306,    26,   198,  4360,   484,   892,   356,   389,  1165,\n",
       "        13674,    25,   262, 10904,  1108,   326,   198,  2001, 42267,   514,\n",
       "           11,   262,  2134,   286,   674, 24672,    11,   318,   355,   281,\n",
       "          198, 24807,   284,  1948,   786,   511, 20038,    26,   674,   198,\n",
       "           82, 13712,   590,   318,   257,  4461,   284,   606,  3914,   514,\n",
       "        15827,   428,   351,   198,   454,   279,  7938])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#down the road we want the transformer to train at each subsequent token\n",
    "#e.g. 18 -> 47, 18 + 47 -> 56, 18 + 47 + 56 -> 58, etc. (see next kernel)\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4525c5b1-2b39-4671-b48b-510dca5c0a60",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([5962]), the target is: 22307\n",
      "when input is tensor([ 5962, 22307]), the target is: 25\n",
      "when input is tensor([ 5962, 22307,    25]), the target is: 198\n",
      "when input is tensor([ 5962, 22307,    25,   198]), the target is: 8421\n",
      "when input is tensor([ 5962, 22307,    25,   198,  8421]), the target is: 356\n",
      "when input is tensor([ 5962, 22307,    25,   198,  8421,   356]), the target is: 5120\n",
      "when input is tensor([ 5962, 22307,    25,   198,  8421,   356,  5120]), the target is: 597\n",
      "when input is tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597]), the target is: 2252\n",
      "when input is tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252]), the target is: 11\n",
      "when input is tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11]), the target is: 3285\n"
     ]
    }
   ],
   "source": [
    "#training like this helps with computational efficiency but also to help expose the transformer\n",
    "#to more context from 1 - blocksize. Needs to get used to seeing everything in between 1-block size\n",
    "#Should also be noted that transformer will NEVER predict based on series of tokens > blocksize (only from 1-bs)\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(10):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context}, the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21e216-1c01-4aaa-8c34-f888c4221711",
   "metadata": {},
   "source": [
    "## Batch Dimensions\n",
    "Going to have chunks of text encodings stacked up in a single tensor for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1956411-a829-47dd-b393-efb59abcbddc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_size = block_size\n",
    "\n",
    "def get_batch(split: str):\n",
    "    #generate small batch of data of inputs x with targets to predict y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate batch_size-sized random indices to get data from\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # for each randomly selected index, get the associated context (up to block size) in the data\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # get offset by so x can try to predict it\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad() #don't update on backwards - more memory efficient\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() #effectively average across many iterations of our training to smooth out loss\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a07d6c34-4259-4af0-94a9-47b92821ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19d26347-5029-49eb-a4df-2215bc2878e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49766, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(test_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cc06a0c-b3b3-4fc8-bac4-95695e430cf8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([16, 256])\n",
      "tensor([[  994,   284,  7284,  ...,  1961, 39743,    25],\n",
      "        [  198,    39, 11262,  ...,   757,    25,   198],\n",
      "        [ 3470,  7319, 35481,  ...,   198,   198, 11473],\n",
      "        ...,\n",
      "        [  198, 13828,   257,  ...,  6487, 22122,    13],\n",
      "        [  465,  3656,  1194,  ...,    13,   198,   198],\n",
      "        [  899,   455,   284,  ...,   550,   355,   300]], device='cuda:0')\n",
      "targets: torch.Size([16, 256])\n",
      "tensor([[  284,  7284, 14566,  ..., 39743,    25,   198],\n",
      "        [   39, 11262, 20754,  ...,    25,   198,    40],\n",
      "        [ 7319, 35481,    13,  ...,   198, 11473,  3843],\n",
      "        ...,\n",
      "        [13828,   257,  6735,  ..., 22122,    13,   198],\n",
      "        [ 3656,  1194,    26,  ...,   198,   198, 49275],\n",
      "        [  455,   284,   198,  ...,   355,   300,  2086]], device='cuda:0') \n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = get_batch('train')\n",
    "print('inputs:', x_batch.shape)\n",
    "print(x_batch)\n",
    "\n",
    "print('targets:', y_batch.shape)\n",
    "print(y_batch, \"\\n\\n----------\\n\")\n",
    "\n",
    "# for batch in range(batch_size):\n",
    "#     for time in range(block_size):\n",
    "#         context = x_batch[batch,:time+1]\n",
    "#         target = y_batch[batch,time]\n",
    "#         print(f'When input is {context}, target is {target}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52c815d7-634b-4d9e-8336-4e05f22d8f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50258"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50933ded-538c-4ef4-9d03-166a6ba37584",
   "metadata": {},
   "source": [
    "## Define Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ccc555c-3b7e-43fd-93af-7a44682250b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #creates the triangular matrix of 1s that makes our mask\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # B,T,16 (head size)\n",
    "        q = self.query(x) # B,T,16 (head size)\n",
    "        # Now we have B, T, 16 tensors where B and T are in parallel - no communication\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # B,T,16 @ B,16,T ----> B,T,T - Weighted aggregation now is a function of the keys and queries of these \n",
    "                                                # Scaled attention to preserve variance\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # B,T,T\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x) # what gets aggregated for the purpose of this single head\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList((Head(head_size) for _ in range(num_heads)))\n",
    "        self.projection = nn.Linear(n_embd, n_embd) #linear transformation of the outcome of forward\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) #concatenate outputs for as many heads as we want working in parallel\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self,n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # projection layer going back into residual pathway\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x) #makes it so that all tokens experience the activation functions independently\n",
    "                           #self attention is the communication, and this feedforward is the opportunity for individual tokens to think on what they learned\n",
    "    \n",
    "\n",
    "class Block(nn.Module): \n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    #intersperses comm. and comp.\n",
    "    #communication done by the multiheaded self-attention and the computation done by the feedforward network on all tokens independently\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # n_head heads of head_size-dimensional self-attention\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embd)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #just adding blocks by themselves increases complexity / how deep the network is with basically the same results. Need to work on the optimization issues\n",
    "        x = x + self.sa(self.layernorm1(x)) # 'x = x + 'allows for residual connections while layernorm 1 and 2 allow for normalized rows before self-attention and feedforward\n",
    "        x = x + self.ffwd(self.layernorm2(x))\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "class BigramLanguageModel(nn.Module): #subclass of nn Module\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__() #call parent's constructor\n",
    "        # each token directly reads off the logits for the next token from lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # create vocab_size x vocab_size embedding table\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])   #intersperse communication and computation 3 times with the 3 blocks\n",
    "        self.layernorm_final = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        # print(torch.max(index))\n",
    "        #index and targets are both (B,T) tensor of integers\n",
    "        #when we pass an index here, every integer in our input will refer to the embedding table\n",
    "        #and pluck out a corresponding row in the table according to its index\n",
    "        #e.g. when we hand it 25 (the encoding of 'M'), it goes to row 25 in embedding table\n",
    "        #then pytorch will arrange it into a Batch x Time x Channel tensor \n",
    "        # Hence, our logits end up as being 4 (batch_size for parallel processing) by 8 (# of context places / block_size) by 65 (vocab_size)\n",
    "        #(B,T,C). Remember, logits are just log counts of a distribution\n",
    "        #logits are our scores for the next character in the sequence\n",
    "            \n",
    "            \n",
    "        tok_emb = self.token_embedding_table(index) #B,T,C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T,device=device)) #T,C\n",
    "        x = tok_emb + pos_emb # B,T,C\n",
    "        x = self.blocks(x) #apply one head of self attention B,T,C\n",
    "        x = self.layernorm_final(x)\n",
    "        logits = self.lm_head(x) #B,T,vocab_size\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None #because there's nothing to aim for\n",
    "        \n",
    "        else:\n",
    "            #torch gets angry if we give it C in the third dimension so we need to reshape\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # reshape into a 32 x 65 tensor\n",
    "            targets = targets.view(B*T)\n",
    "            #need to evaluate the loss function\n",
    "            # let's use negative log likelihood / crossentropy\n",
    "            loss = F.cross_entropy(logits,targets) # how well are we predicting next character based on the logits?\n",
    "                                                   # ideally, the correct dimension (point at 4,2,45 for example) should have a very high number while others are low\n",
    "        return logits, loss\n",
    "\n",
    "    #Essentially a 'predict' function that 'generates' new \n",
    "     #index is (Batchsize,Time) tensor of integers in current context\n",
    "                                                    #max_new_tokens is max number of tokens to generate (?)\n",
    "    def generate(self, index, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            #crop index to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:] # helps with the positional encoding - don't feed info we don't have access to\n",
    "            \n",
    "            #get predictions\n",
    "            logits, loss = self(index_cond) # calls forward\n",
    "            #focus on only last time step (want the most trained version of the model)\n",
    "            logits = logits[:, -1, :] \n",
    "            #apply softmax to get probabilities (exponentiate to approximate counts then get proportions to approximate probabilities)\n",
    "            probs = F.softmax(logits, dim=-1) # B,C\n",
    "            #sample from the distribution to get next character\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # B, 1, in each batch dimension we have a single prediction of what comes next\n",
    "            #append sampled index to running sequence given current context of what we've predicted before\n",
    "            index = torch.cat((index,index_next),dim=1) # B, T+1\n",
    "        return index\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85161d3a-a717-4dd2-805f-eb7fa1cc06b0",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a0beac6-7781-435c-92f7-7ab005df5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e8a5c40-dcf7-4c96-937c-53bc7ba9a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(decode(m.generate(index = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=500)[0].tolist()))\n",
    "#Gives us garbage because our current model is only looking at the last character bc it's a bigram model\n",
    "#Also not trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99f63b-66a6-4a03-88ad-b248cc23e31b",
   "metadata": {},
   "source": [
    "## Let's Optimize and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50ae5c41-7b61-4b0a-8c5d-a37f65d64a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edca7b79-3185-44aa-befd-e1ee417766fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.9655, val loss 10.9779\n",
      "step 500: train loss 4.4067, val loss 5.0260\n",
      "step 1000: train loss 3.8065, val loss 4.8241\n",
      "step 1500: train loss 3.3984, val loss 4.7467\n",
      "step 2000: train loss 2.9876, val loss 4.8476\n",
      "step 2500: train loss 2.6160, val loss 4.9518\n",
      "step 3000: train loss 2.2462, val loss 5.1589\n",
      "step 3500: train loss 1.8781, val loss 5.3735\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 12\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:305\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    303\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 305\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    307\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for steps in range(max_iters):\n",
    "     # every once in a while evaluate the loss on train and val sets\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    #sample batch of data\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    #evaluate loss\n",
    "    logits, loss = m(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b65c46b9-89fa-46c1-8227-24596d5b26eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! heaven were not thy stone;\n",
      "A pack up the head of pierce like death:\n",
      "Nay, for less adieu; I'll this.\n",
      "FRIAR LAURENCE: stand between\n",
      "And stony entrance to thee where hast thou not pleased out.\n",
      "What! the molehill for Rosaline canker change?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Can sooniness, and rude, knee;\n",
      "Not sleeping kill'd.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "What dreadful noise of him?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Away, nor quarrel else?\n",
      "As near, great as I love him that you on you;\n",
      "I am too well in mercy:\n",
      "Give me too horrible conceit presently,\n",
      "And you can you beg of justice did displace:\n",
      "Give me assurance with you in this fact,\n",
      "And yet I do call it excellent.\n",
      "Have you, brother being moved,\n",
      "Who pass'd, good design, to his prince and himself to not before you!\n",
      "NorLORD ROSS:\n",
      "God will her Richard's eyes? will not do it so,\n",
      "\n",
      "As I fear; therefore, not break the executioner wherein\n",
      "Lest that this the air on thieves.\n",
      "We do this break thine;\n",
      "Or here we sto from the affection.\n",
      "\n",
      "LORD WILLOUGHBY:\n",
      "I would please the same is to speak\n",
      "In his good-place;\n",
      "The father owning it will in it.\n",
      "\n",
      "LORD WILLOUGHBY:\n",
      "And I will have heard it out his power, in hope, and therefore prepare\n",
      "The earldom and bestow a wrong.\n",
      "\n",
      "BUSHY:\n",
      "The dangerous and so I brookest presence.\n",
      "\n",
      "LORD WILLOUGHBY:\n",
      "Base and sup betimes;\n",
      "Richard for myself that thou drow;\n",
      "God be all unto the duke'st not thou object,\n",
      "As here I purchased this terror,\n",
      "Were not stand affected the way, infold.\n",
      "\n",
      "QUEEN:\n",
      "That would you have; and thou art against me well, because I did wish\n",
      "With curses where Echos that thou wert cause to tell?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Comforty star hath done a subject, there is left with aere death;\n",
      "So many a furrow;\n",
      "But one being close by out the other sportfulfilled:\n",
      "Better it have I out my place.\n",
      "\n",
      "RIVERS:\n",
      "O, Rivers, marry, I have nothing, be brief to thee here?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "So now the crown of it with thee;\n",
      "One, though I did love myself, if I had,\n",
      "And God will never wish it out ripe tide.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "I would I indeed, which I were murderousrow to the walls,\n",
      "Clare the sanctuary, to be ta'en by my nature.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Thus do out a loss, you and you lie on my swift\n",
      "That it may close, Margaret\n",
      "My soul stir him with all wrong,\n",
      "Where wretched by this the contrary dares the crown?\n",
      "Or, is this our carn waters from danger?\n",
      "Edward, cheer you, nephew, bear you are shallow pluck the King Henry what likelihood of you!\n",
      "Teach not so; and his sons?\n",
      "Our faith, and  may sack;\n",
      "Or if Saint George from whence shall never to see him,\n",
      "No faith to the breath of my Gloucester here,\n",
      "And graculate the throne here.\n",
      "\n",
      "JOHN OF GAUNT:\n",
      "Thou, I must be rid of this presence,\n",
      "As little cost my wrongs may set the king\n",
      "Is the wrong to bitter;\n",
      "Which heavy looks endure him any danger may compare:\n",
      "Then, so before the way, by the life as we thought\n",
      "At men of the king was survey to him,\n",
      "As mine own part of him, twenty power: then, thou wilt\n",
      "Had he of justice seem'd before his way.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Yet, I would he had beg, to speak to the writing.\n",
      "Thou art the prisoner to Bolingbroke,\n",
      "Wilt thou be prevented to me?\n",
      "As who dale:\n",
      "But thou dost guess so much on Hereford and weep,\n",
      "And thou, how near the foe'st and me smear grace good thee;\n",
      "Or I with death, to make thee'll renew,\n",
      "And throw away that usurp the heavy day of:\n",
      "And I in thy foul misleading me up my head;\n",
      "And die, thou delight to pass the rest;\n",
      "And in mine enemies, I will my lady,\n",
      "Shame to my intents do stay aetch\n",
      "In vain comes?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Henry now have kept both these old stables:\n",
      "Edward, God sayAs if thou were this, Warwick teach,\n",
      "I would speak thy office from the penitent parts and thee.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "So to right royal root not on Henry, Hastings;\n",
      "Hath turn our daughter come;\n",
      "Since thou art flying to me; for have to this is ominous to this.\n",
      "\n",
      "HEN ELIZABETH:\n",
      "O, he may chance to the father he would labour his head is past.\n",
      "\n",
      "GLOUCESTER:\n",
      "How would these our swift?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "The heavens have is prophet dearly.\n",
      "To him bring this innocent.\n",
      "\n",
      "KING HENRY Bion, to do some little innocent.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Edward, who shall hinder him will?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "So what you wrangling him here you being close all.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Dorset, my king; there's faces about the mother duke,\n",
      "And beg thy health and hereafter be past.\n",
      "\n",
      "KING RICHARD III:\n",
      "I leave thee with welcome thee then:\n",
      "They shall I pray at to despair, madam;\n",
      "And I will do repent the no harm.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Come, that must to rest presently.\n",
      "\n",
      "RIVERS:\n",
      "No after God as I am hounds\n",
      "To set her soul to end thy bed.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Hath now upon that title by new\n",
      "To leave this brave wrong.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "May she may do I were dead;\n",
      "But for a loss may, toward such a night.\n",
      "\n",
      "KING RICHARD III:\n",
      "Then, then my son were daughter in the sea puff\n",
      "That none can; if subject can I intend to joy,\n",
      "So many a thousand prayers two for my ordinance con.\n",
      "What if this at his death himself,\n",
      "Thou should lamenting what a need we fear\n",
      "Whose western coast to glance being butchequer,\n",
      "And with the king unto the white rose and waters'uck but hold close a shame,\n",
      "So I'll open the last. I have news, how well-Upon loves thee\n",
      "true descent.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "My soul awhile, and heart is gone, fair Richard must I were to be in mine eye.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "The devil shall hinder life before my soul to hear.\n",
      "\n",
      "KING RICHARD III:\n",
      "We charge your liege.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "I come. Write to offence.\n",
      "Meantime.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "No, therefore mistrust me thy kindness I to thee.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "So now the time will I rise thus?\n",
      "\n",
      "KING RICHARD III:\n",
      "Thou troublest me that I were past: here, myself have done this innocent of thy sons,\n",
      "To Henry with telling by their tender bab deserves before.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "I, O thou hadst thou cozeth me,\n",
      "I'll not mark'd it in mine eyes.\n",
      "Here in the world not mean the midwife be done I did indeed\n",
      "Do by that holp to kill my sister;\n",
      "Or to-day, O, there am too much bad?\n",
      "Is thyself, proud with my way toad,\n",
      "I am the if with a style,\n",
      "Or, here I with thee of what I have lie.\n",
      "But let this call'd from this earth the king,\n",
      "When Gaunt! plant thy speech height\n",
      "My sons and Edward, hath made these arms\n",
      "Seeking for Edward,\n",
      "And leave his resolution by his poor Clifford's pent heart.\n",
      "\n",
      "KING RICHARD III:\n",
      "Nay, thou I shrors, and that by the soul's purge\n",
      "I know some pity prove him withal.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "And now shall I live there, I die at my life.\n",
      "\n",
      "KING RICHARD III:\n",
      "What!\n",
      "Wdraw thee?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Even when a husband's gift steepful tear by the title!\n",
      "The damned spilt thou let it me?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "And yet I take my wrongs that dear meat have begun,\n",
      "And by their spite of\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(index = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a245fec5-c53f-44ad-919d-7b5aae921844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "torch.save(m.state_dict(), 'gpt-model-shakespeare-subword.pth')\n",
    "torch.save(m.state_dict(), 'gpt-model-state-dict-shakespeare-subword.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f544c-78eb-4ffb-902c-b99eb69c8d6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mathematical Trick in Self-Attention - 3 Methods\n",
    "How Transformers differ from traditional LSTM / RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675804fa-38d9-4579-be32-088aa28d4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 #batch, time, channels\n",
    "#Review Batch is for parallel processing, time is the amount of context, channels are the characters that can be predicted\n",
    "\n",
    "#For predicting, information should only flow from previous context to current. It should not take from future tokens in training (e.g. In the word 'Hamburger,'\n",
    "#it shouldn't use 'u' as influence for choosing 'b')\n",
    "\n",
    "#The easiest way for tokens to communicate is to do an average of all the preceding elements\n",
    "#You can then add that average of previous context as a feature vector what you already know\n",
    "#Recognize that an average is a very weak/lossy way of summarizing info, but the principle of summarizing what you already know into\n",
    "#a number that represents previous context is key\n",
    "x = torch.randn(B,T,C) #fill 4,8,2 tensor with random numbers\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265f90f-4973-4927-85b1-1b2e941f1d22",
   "metadata": {},
   "source": [
    "### Self-Attention Mask 1 - Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99da2a-4a2e-42fe-949e-716079bf65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[batch, time] = mean_{i<=t} x[b,i]\n",
    "x_bag_of_words = torch.zeros((B,T,C))\n",
    "for b in range(B): #iterate over batch dimensions\n",
    "    for t in range(T): #iterate over time\n",
    "        x_prev = x[b,:t+1] #(t,C)\n",
    "        x_bag_of_words[b,t] = torch.mean(x_prev, 0) #averaging out previous x's over time\n",
    "\n",
    "print(x[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80468757-367d-4753-bd96-eb18c75d4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_bag_of_words[0])\n",
    "#each element is an average of prev elements in tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fbfe0b-104f-4c46-9da3-125dadbca7c4",
   "metadata": {},
   "source": [
    "### Self-Attention Mask 2 - Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0dad0-5064-4a4b-a315-cc18600489fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication basics\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "print(\"Works by multiplying first row of a by first col of b and adding up (1*2 + 1*6 + 1*6 = 14), same thing for 16 (7+4+5) etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15292d71-fe22-4995-af7a-457bd62433e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3)) # gives lower triangular part of matrix\n",
    "#as you progress down the matrix, you progressively ignore 1 less element of b due to the growing # of 1s and shrinking number of 0s\n",
    "a /= torch.sum(a, 1, keepdim=True)\n",
    "#now we are able to average the sums going down rows because for each row we are essentially multiplying it by 1/row_num (which is an average)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182ea26-a08b-4db0-967f-a60297b83ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights / weights.sum(1, keepdim = True)\n",
    "x_bag_of_words_2 = weights @ x # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "# xbow and xbow2 should be equal but this way is MUCH faster\n",
    "# essentially we are doing weighted sums by using the triangular torch.tril so that we can only have the matrix access\n",
    "# only the tokens preceding it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29369b51-5a9e-4cb1-a5fb-019a1419f5b3",
   "metadata": {},
   "source": [
    "### Self-Attention Mask 3 - Using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9821b-d4a4-4913-aca7-d220999c1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #fill zeros with -inf\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and average across each row -> end up with same matrix as the previous two methods\n",
    "x_bag_of_words_3 = wei @ x\n",
    "x_bag_of_words_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e53274-a2f9-48f1-9ed9-4ce9aa338743",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Self-Attention Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e5898-fe83-4104-8a56-de2690584a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn((B,T,C))\n",
    "\n",
    "#emit a query and a key vector\n",
    "#Query vector - what am I looking for\n",
    "#Key vector - what do i contain\n",
    "#obtain affinities between vectors by effectively doing a dot product between the keys and the queries\n",
    "# e.g. A query dot products with all the keys and that dot product becomes wei\n",
    "\n",
    "#Single head that performs self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # B,T,16 (head size)\n",
    "q = query(x) # B,T,16 (head size)\n",
    "# Now we have B, T, 16 tensors where B and T are in parallel - no communication\n",
    "wei = q @ k.transpose(-2, -1) # B,T,16 @ B,16,T ----> B,T,T - Weighted aggregation now is a function of the keys and queries of these nodes\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T)) # don't want it to be all uniform because some tokens will have natural affinities for or against other tokens in the past (this currently doesn't do that)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x) # what gets aggregated for the purpose of this single head\n",
    "out = wei @ v \n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904f22d-ba9e-48a2-973c-8bd1d159693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei[0] #now wei is data dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c043f5-34fe-4547-881f-e42938fe98ed",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Attention is a **communication mechanism.** Can be seen as nodes in a directed graph looking at each other and aggregating into a single value with a weighted sum from all nodes that point to them with data-dependent weights \n",
    "- There is no notion of space\n",
    "- There is no communication across batch dimensions\n",
    "- Attention block doesn't necessarily have to only communicate backwards (E.g. Token at Pos 5 will never look forward to Pos 6 to choose what token it should be). In some cases you may want to have all tokens talk to each other (e.g. sentiment analysis where future context words may shed a different light on previous ones). To create such an encoder block, all you have to do is delete wei = wei.masked_fill(tril == 0, float('-inf')), because that removes the mask of -inf and allows for aggregates at all Ts regardless of temporal position\n",
    "    - What we have currently is called a decoder block\n",
    "- There is also something called cross-attention. This block of code is called self-attention because the keys, queries, and values are all coming from the same source (x). However, there can be a case where your queries come from x but key and values come from another source. \n",
    "    - Called cross attention if there is a separate pool of nodes that we want to pool information from \n",
    "- Scaled self attention additionally divides wei by 1/sqrt(head_size) - this makes it so when input Query, Key are dot producted, the resulting variance will be preserved to approximately 1 as opposed to the order of head_size\n",
    "    - Prevents softmax from being way too peaky because with multiplication it tends to optimize towards the max values, leading to high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723b153-f96b-4478-9107-4809f704124e",
   "metadata": {},
   "source": [
    "# Layernorm Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99bbff-5c98-49a7-a4e8-3b7a6eba4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "    \"\"\" normalize rows to 1 stdev\"\"\"\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "  \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e61f74c-3511-4521-af7d-9e8d4ccd7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,:].mean(), x[0,:].std() # the ROWS are normalized to stdev of 1 and mean of 0 (normal distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399edc0-a651-45fd-a9b2-d30220efcf6b",
   "metadata": {},
   "source": [
    "# Generate Infinite Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2d4a9-f947-4889-8cc3-3f0990dab496",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load('gpt-model-shakespeare.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0c1d9-f775-412c-992f-9de6a0066c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = decode(model2.generate(index = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=2000)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e6aef-814c-4c68-93b5-261bcd30fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare_output.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(model_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
