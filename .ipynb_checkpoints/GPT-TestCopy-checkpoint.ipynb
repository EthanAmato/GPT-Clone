{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8dec6c-b6f5-4e22-93ea-95eb5c48dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a35a8-e44c-44f3-a5fd-19bd23151b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset is  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the dataset is \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c3547c-d2de-442f-b5f4-65bd532795fe",
   "metadata": {},
   "source": [
    "#### First 1,000 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84581b2b-7b01-46a0-87e1-8df2594a8824",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc8e35-987d-48bb-aa8c-f6437f6f2c3e",
   "metadata": {},
   "source": [
    "#### Find unique character and how many there are\n",
    "Note that this is a character level tokenizer which is very simple and that in practice\n",
    "people will use subword-level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75cae0-e844-4254-99f8-8d11c79a8b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset includes the following characters:\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "This dataset has 65 unique characters\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "vocab = ''.join(chars) \n",
    "\n",
    "print(f'This dataset includes the following characters:{vocab}\\n')\n",
    "print(f'This dataset has {vocab_size} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e50b3c-9afd-4081-a766-36c715a07198",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cac667-5b03-417b-904c-f57d70f4b937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 17, 58, 46, 39, 52]\n",
      "My name is Ethan\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from chars to integers\n",
    "stoi = {char:i for i,char in enumerate(chars)} #based on index in our sorted array of unique chars, assign number to each character in dictionary (for encoding)\n",
    "itos =  {i:char for i,char in enumerate(chars)} #do the same thing but have index as key and char as value (for decoding)\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # given string s, return an array of ints that pertain to each character\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # given array of integers, decode into chars using itos and turn into string\n",
    "\n",
    "print(encode(\"My name is Ethan\"))\n",
    "print(decode(encode(\"My name is Ethan\")))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17808b-7dcd-41bd-8ae3-43ba3643f4ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Import modules and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f90ac-723c-44fc-9081-86d2cd482b65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch # lets use pytorch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9456ff-f6d3-44f5-90a8-98a44e9d3bfa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10aea9d-bff8-48eb-b8c1-813ded0b09ee",
   "metadata": {},
   "source": [
    "## Separate into train and test validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51ab21-5850-4c33-b7b9-816b30d7baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data)) # 90% train x 10% test split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36152d91-6b5b-4c2a-bb02-810aa88348ac",
   "metadata": {},
   "source": [
    "## Set block size for chunking\n",
    "train in chunks of n tokens at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e19c474-c006-4ef4-b22c-94f39fa5b781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#down the road we want the transformer to train at each subsequent token\n",
    "#e.g. 18 -> 47, 18 + 47 -> 56, 18 + 47 + 56 -> 58, etc. (see next kernel)\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4525c5b1-2b39-4671-b48b-510dca5c0a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), the target is: 47\n",
      "when input is tensor([18, 47]), the target is: 56\n",
      "when input is tensor([18, 47, 56]), the target is: 57\n",
      "when input is tensor([18, 47, 56, 57]), the target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "#training like this helps with computational efficiency but also to help expose the transformer\n",
    "#to more context from 1 - blocksize. Needs to get used to seeing everything in between 1-block size\n",
    "#Should also be noted that transformer will NEVER predict based on series of tokens > blocksize (only from 1-bs)\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context}, the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21e216-1c01-4aaa-8c34-f888c4221711",
   "metadata": {},
   "source": [
    "## Batch Dimensions\n",
    "Going to have chunks of text encodings stacked up in a single tensor for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1956411-a829-47dd-b393-efb59abcbddc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]]) \n",
      "\n",
      "----------\n",
      "\n",
      "When input is tensor([24]), target is 43.\n",
      "When input is tensor([24, 43]), target is 58.\n",
      "When input is tensor([24, 43, 58]), target is 5.\n",
      "When input is tensor([24, 43, 58,  5]), target is 57.\n",
      "When input is tensor([24, 43, 58,  5, 57]), target is 1.\n",
      "When input is tensor([24, 43, 58,  5, 57,  1]), target is 46.\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46]), target is 43.\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46, 43]), target is 39.\n",
      "When input is tensor([44]), target is 53.\n",
      "When input is tensor([44, 53]), target is 56.\n",
      "When input is tensor([44, 53, 56]), target is 1.\n",
      "When input is tensor([44, 53, 56,  1]), target is 58.\n",
      "When input is tensor([44, 53, 56,  1, 58]), target is 46.\n",
      "When input is tensor([44, 53, 56,  1, 58, 46]), target is 39.\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39]), target is 58.\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39, 58]), target is 1.\n",
      "When input is tensor([52]), target is 58.\n",
      "When input is tensor([52, 58]), target is 1.\n",
      "When input is tensor([52, 58,  1]), target is 58.\n",
      "When input is tensor([52, 58,  1, 58]), target is 46.\n",
      "When input is tensor([52, 58,  1, 58, 46]), target is 39.\n",
      "When input is tensor([52, 58,  1, 58, 46, 39]), target is 58.\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58]), target is 1.\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]), target is 46.\n",
      "When input is tensor([25]), target is 17.\n",
      "When input is tensor([25, 17]), target is 27.\n",
      "When input is tensor([25, 17, 27]), target is 10.\n",
      "When input is tensor([25, 17, 27, 10]), target is 0.\n",
      "When input is tensor([25, 17, 27, 10,  0]), target is 21.\n",
      "When input is tensor([25, 17, 27, 10,  0, 21]), target is 1.\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1]), target is 54.\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]), target is 39.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 #how many independent sequences will be processed at once\n",
    "block_size = block_size\n",
    "\n",
    "def get_batch(split: str):\n",
    "    #generate small batch of data of inputs x with targets to predict y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate batch_size-sized random indices to get data from\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # for each randomly selected index, get the associated context (up to block size) in the data\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # get offset by so x can try to predict it\n",
    "    return x, y\n",
    "x_batch, y_batch = get_batch('train')\n",
    "print('inputs:', x_batch.shape)\n",
    "print(x_batch)\n",
    "\n",
    "print('targets:', y_batch.shape)\n",
    "print(y_batch, \"\\n\\n----------\\n\")\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    for time in range(block_size):\n",
    "        context = x_batch[batch,:time+1]\n",
    "        target = y_batch[batch,time]\n",
    "        print(f'When input is {context}, target is {target}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50933ded-538c-4ef4-9d03-166a6ba37584",
   "metadata": {},
   "source": [
    "## Begin with making Bigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ccc555c-3b7e-43fd-93af-7a44682250b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.7691, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJt-wBpm&yiltNCjeO3:Cx&vvMYW-txjuAd IRFbTpJ$zkZelxZtTlHNzdXXUiQQY:qFINTOBNLI,&oTigq z.c:Cq,SDXzetn3XVjX-YBcHAUhk&PHdhcOb\n",
      "nhJ?FJU?pRiOLQeUN!BxjPLiq-GJdUV'hsnla!murI!IM?SPNPq?VgC'R\n",
      "pD3cLv-bxn-tL!upg\n",
      "SZ!Uvdg CtxtT?hsiW:XxKIiPlagHIsr'zKSVxza?GlDWObPmRJgrIAcmspmZ&viCKot:u3qYXA:rZgv f:3Q-oiwUzqh'Z!I'zRS3SP rVchSFUIdd q?sPJpUdhMCK$VXXevXJFMl,i\n",
      "YxA:gWId,EXR,iMC,$?srV$VztRwb?KpgUWFjR$zChOLm;JrDnDph\n",
      "LBj,KZxJa\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module): #subclass of nn Module\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__() #call parent's constructor\n",
    "        # each token directly reads off the logits for the next token from lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # create vocab_size x vocab_size embedding table\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        #index and targets are both (B,T) tensor of integers\n",
    "        #when we pass an index here, every integer in our input will refer to the embedding table\n",
    "        #and pluck out a corresponding row in the table according to its index\n",
    "        #e.g. when we hand it 25 (the encoding of 'M'), it goes to row 25 in embedding table\n",
    "        #then pytorch will arrange it into a Batch x Time x Channel tensor \n",
    "        # Hence, our logits end up as being 4 (batch_size for parallel processing) by 8 (# of context places / block_size) by 65 (vocab_size)\n",
    "        logits = self.token_embedding_table(index) #(B,T,C). Remember, logits are just log counts of a distribution\n",
    "            #logits are our scores for the next character in the sequence\n",
    "        if targets is None:\n",
    "            loss = None #because there's nothing to aim for\n",
    "        \n",
    "        else:\n",
    "            #torch gets angry if we give it C in the third dimension so we need to reshape\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # reshape into a 32 x 65 tensor\n",
    "            targets = targets.view(B*T)\n",
    "            #need to evaluate the loss function\n",
    "            # let's use negative log likelihood / crossentropy\n",
    "            loss = F.cross_entropy(logits,targets) # how well are we predicting next character based on the logits?\n",
    "                                                   # ideally, the correct dimension (point at 4,2,45 for example) should have a very high number while others are low\n",
    "        return logits, loss\n",
    "\n",
    "    #Essentially a 'predict' function that 'generates' new \n",
    "     #index is (Batchsize,Time) tensor of integers in current context\n",
    "                                                    #max_new_tokens is max number of tokens to generate (?)\n",
    "    def generate(self, index, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            #get predictions\n",
    "            logits, loss = self(index) # calls forward\n",
    "            #focus on only last time step (want the most trained version of the model)\n",
    "            logits = logits[:, -1, :] # becomes B, C. The -1 eliminates the t dimension and we are looking at the most recent\n",
    "            #apply softmax to get probabilities (exponentiate to approximate counts then get proportions to approximate probabilities)\n",
    "            probs = F.softmax(logits, dim=-1) # B,C\n",
    "            #sample from the distribution to get next character\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # B, 1, in each batch dimension we have a single prediction of what comes next\n",
    "            #append sampled index to running sequence given current context of what we've predicted before\n",
    "            index = torch.cat((index,index_next),dim=1) # B, T+1\n",
    "        return index\n",
    "                \n",
    "    \n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(x_batch, y_batch)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "\n",
    "print(decode(m.generate(index = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n",
    "#Gives us garbage because our current model is only looking at the last character bc it's a bigram model\n",
    "#Also not trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a99f63b-66a6-4a03-88ad-b248cc23e31b",
   "metadata": {},
   "source": [
    "### Let's Optimize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50ae5c41-7b61-4b0a-8c5d-a37f65d64a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "edca7b79-3185-44aa-befd-e1ee417766fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.475322961807251\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    #sample batch of data\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    #evaluate loss\n",
    "    logits, loss = m(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b65c46b9-89fa-46c1-8227-24596d5b26eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ou\n",
      "HANI borong s bl?\n",
      "DW:\n",
      "Anto' h bls whoommeulye Werngss myou\n",
      "She\n",
      "BESCONG RDoleshet heked\n",
      "GUE t at, sours s y par 's h, de,\n",
      "I hathago pe, wowhe w?\n",
      "\n",
      "I we be lirewin ad:\n",
      "Myorodeter t ceom,\n",
      "\n",
      "I'sustier IORCENGJULI dow hof, issonse thel becepin.\n",
      "sthtour Gom:\n",
      "\n",
      "\n",
      "\n",
      "Cannof k tigr tin mame\n",
      "Wint pHandenomety rtonossoite pll t dsuneroncu ren hinder's,\n",
      "sapp, her waige het, andanseve otoe, ngr are? tanol hat'seo hinee tiredsthorithly t ishe.\n",
      "FRonis l omyo acem y r, I matolist, ay then desous s crs!\n",
      "\n",
      "RWhathin c\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(index = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f544c-78eb-4ffb-902c-b99eb69c8d6a",
   "metadata": {},
   "source": [
    "### Mathematical Trick in Self-Attention\n",
    "How Transformers differ from traditional LSTM / RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "675804fa-38d9-4579-be32-088aa28d4f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 #batch, time, channels\n",
    "#Review Batch is for parallel processing, time is the amount of context, channels are the characters that can be predicted\n",
    "\n",
    "#For predicting, information should only flow from previous context to current. It should not take from future tokens in training (e.g. In the word 'Hamburger,'\n",
    "#it shouldn't use 'u' as influence for choosing 'b')\n",
    "\n",
    "#The easiest way for tokens to communicate is to do an average of all the preceding elements\n",
    "#You can then add that average of previous context as a feature vector what you already know\n",
    "#Recognize that an average is a very weak/lossy way of summarizing info, but the principle of summarizing what you already know into\n",
    "#a number that represents previous context is key\n",
    "x = torch.randn(B,T,C) #fill 4,8,2 tensor with random numbers\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1d99da2a-4a2e-42fe-949e-716079bf65f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n"
     ]
    }
   ],
   "source": [
    "# We want x[batch, time] = mean_{i<=t} x[b,i]\n",
    "x_bag_of_words = torch.zeros((B,T,C))\n",
    "for b in range(B): #iterate over batch dimensions\n",
    "    for t in range(T): #iterate over time\n",
    "        x_prev = x[b,:t+1] #(t,C)\n",
    "        x_bag_of_words[b,t] = torch.mean(x_prev, 0) #averaging out previous x's over time\n",
    "\n",
    "print(x[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80468757-367d-4753-bd96-eb18c75d4929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "print(x_bag_of_words[0])\n",
    "#each element is an average of prev elements in tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6ad0dad0-5064-4a4b-a315-cc18600489fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "Works by multiplying first row of a by first col of b and adding up (1*2 + 1*6 + 1*6 = 14), same thing for 16 (7+4+5) etc.\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication basics\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "print(\"Works by multiplying first row of a by first col of b and adding up (1*2 + 1*6 + 1*6 = 14), same thing for 16 (7+4+5) etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "15292d71-fe22-4995-af7a-457bd62433e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3)) # gives lower triangular part of matrix\n",
    "#as you progress down the matrix, you progressively ignore 1 less element of b due to the growing # of 1s and shrinking number of 0s\n",
    "a /= torch.sum(a, 1, keepdim=True)\n",
    "#now we are able to average the sums going down rows because for each row we are essentially multiplying it by 1/row_num (which is an average)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5182ea26-a08b-4db0-967f-a60297b83ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights / weights.sum(1, keepdim = True)\n",
    "x_bag_of_words_2 = weights @ x # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "# xbow and xbow2 should be equal but this way is MUCH faster\n",
    "# essentially we are doing weighted sums by using the triangular torch.tril so that we can only have the matrix access\n",
    "# only the tokens preceding it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29369b51-5a9e-4cb1-a5fb-019a1419f5b3",
   "metadata": {},
   "source": [
    "### Using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "42f9821b-d4a4-4913-aca7-d220999c1fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #fill zeros with -inf\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and average across each row -> end up with same matrix as the previous two methods\n",
    "x_bag_of_words_3 = wei @ x\n",
    "x_bag_of_words_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8811e70-a717-479e-967e-da73a11c4a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e5898-fe83-4104-8a56-de2690584a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904f22d-ba9e-48a2-973c-8bd1d159693a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2891e7-ae11-4faa-80bd-ea5ac92fb973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99bbff-5c98-49a7-a4e8-3b7a6eba4a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
